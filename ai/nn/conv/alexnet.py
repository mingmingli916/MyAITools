from keras.models import Sequential
from keras.layers.normalization import BatchNormalization
from keras.layers.convolutional import Conv2D
from keras.layers.convolutional import MaxPooling2D
from keras.layers.core import Dense, Activation, Dropout, Flatten
from keras.regularizers import l2
from keras import backend as K


def AlexNet(width, height, depth, classes, reg=.0002):
    # initialize the model along with the input shape to
    # be "channels last" and the channels dimension itself
    model = Sequential()
    input_shape = (height, width, depth)
    chan_dim = -1

    # if "channels first", update the input shape
    # and channels dimension
    if K.image_data_format() == 'channels_fist':
        input_shape = depth, height, width
        chan_dim = 1

    # Block #1: first CONV => RELU => POOL layer set
    model.add(Conv2D(96,  # filters
                     (11, 11),
                     strides=(4, 4),
                     input_shape=input_shape,
                     padding='same',
                     kernel_regularizer=l2(reg)))
    model.add(Activation('relu'))
    model.add(BatchNormalization(axis=chan_dim))
    model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))
    model.add(Dropout(.25))

    # Block #2: second CONV => RELU => POOL layer set
    model.add(Conv2D(256,
                     (5, 5),
                     padding='same',
                     kernel_regularizer=l2(reg)))
    model.add(Activation('relu'))
    model.add(BatchNormalization(axis=chan_dim))
    model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))
    model.add(Dropout(.25))

    # Block #3: CONV => RELU => CONV => RELU => CONV => RELU => POOL
    model.add(Conv2D(384,
                     (3, 3),
                     padding='same',
                     kernel_regularizer=l2(reg)))
    model.add(Activation('relu'))
    model.add(BatchNormalization(axis=chan_dim))
    model.add(Conv2D(384,
                     (3, 3),
                     padding='same',
                     kernel_regularizer=l2(reg)))
    model.add(Activation('relu'))
    model.add(BatchNormalization(axis=chan_dim))
    model.add(Conv2D(256,
                     (3, 3),
                     padding='same',
                     kernel_regularizer=l2(reg)))
    model.add(Activation('relu'))
    model.add(BatchNormalization(axis=chan_dim))
    model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))
    model.add(Dropout(.25))

    # Block #4: first set of FC => RELU layers
    model.add(Flatten())
    model.add(Dense(4096, kernel_regularizer=l2(reg)))
    model.add(Activation('relu'))
    model.add(BatchNormalization(axis=chan_dim))
    model.add(Dropout(.25))

    # Block #5: second set of FC => RELU layers
    model.add(Dense(4096, kernel_regularizer=l2(reg)))
    model.add(Activation('relu'))
    model.add(BatchNormalization(axis=chan_dim))
    model.add(Dropout(.25))

    # softmax classifier
    model.add(Dense(classes, kernel_regularizer=l2(reg)))
    model.add(Activation('softmax'))

    return model
